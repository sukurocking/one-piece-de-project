{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/23 20:23:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"onepiece\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "file = './onepieceanime/ONE PIECE.csv'\n",
    "df = spark.read.csv(file,header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+------+-------+--------------------+-----+-----------+--------------+\n",
      "|_c0|  rank|trend|season|episode|                name|start|total_votes|average_rating|\n",
      "+---+------+-----+------+-------+--------------------+-----+-----------+--------------+\n",
      "|  0|24,129|   18|     1|      1|I'm Luffy! The Ma...| 1999|        647|           7.6|\n",
      "|  1|29,290|   11|     1|      2|The Great Swordsm...| 1999|        473|           7.8|\n",
      "|  2|32,043|    7|     1|      3|Morgan vs. Luffy!...| 1999|        428|           7.7|\n",
      "|  3|28,818|    8|     1|      4|Luffy's Past! The...| 1999|        449|           8.1|\n",
      "|  4|37,113|    4|     1|      5|Fear, Mysterious ...| 1999|        370|           7.5|\n",
      "|  5|36,209|    4|     1|      6|Desperate Situati...| 1999|        364|           7.7|\n",
      "|  6|37,648|    4|     1|      7|Sozetsu Ketto! Ke...| 1999|        344|           7.7|\n",
      "|  7|38,371|    6|     1|      8|Shousha wa docchi...| 1999|        335|           7.7|\n",
      "|  8|42,249|    5|     1|      9|Seigi no usotsuki...| 2000|        327|           7.3|\n",
      "|  9|41,829|    4|     1|     10|Chijou saikyou no...| 2000|        314|           7.5|\n",
      "+---+------+-----+------+-------+--------------------+-----+-----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/23 20:23:14 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , rank, trend, season, episode, name, start, total_votes, average_rating\n",
      " Schema: _c0, rank, trend, season, episode, name, start, total_votes, average_rating\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/sukumarsubudhi/Downloads/Learning/personal_projects/one-piece-de-project/onepieceanime/ONE%20PIECE.csv\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "958"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- rank: string (nullable = true)\n",
      " |-- trend: string (nullable = true)\n",
      " |-- season: integer (nullable = true)\n",
      " |-- episode: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- start: integer (nullable = true)\n",
      " |-- total_votes: string (nullable = true)\n",
      " |-- average_rating: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the columns like rank, trend, total_votes are imported as string\n",
    "We need to convert them to integer columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the 3 columns to int\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "for colname in [\"rank\", \"trend\", \"total_votes\"]:\n",
    "    df = df.withColumn(colname, F.regexp_replace(F.col(colname), \",\", \"\"))\n",
    "    df = df.withColumn(colname, df[colname].cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/23 20:23:39 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , rank, trend, season, episode, name, start, total_votes, average_rating\n",
      " Schema: _c0, rank, trend, season, episode, name, start, total_votes, average_rating\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/sukumarsubudhi/Downloads/Learning/personal_projects/one-piece-de-project/onepieceanime/ONE%20PIECE.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+-------+----+-----+-----------+--------------+\n",
      "|_c0|rank|trend|season|episode|name|start|total_votes|average_rating|\n",
      "+---+----+-----+------+-------+----+-----+-----------+--------------+\n",
      "+---+----+-----+------+-------+----+-----+-----------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.filter(F.col(\"total_votes\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------+-------+----+-----+-----------+--------------+\n",
      "|_c0|rank|trend|season|episode|name|start|total_votes|average_rating|\n",
      "+---+----+-----+------+-------+----+-----+-----------+--------------+\n",
      "+---+----+-----+------+-------+----+-----+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(F.col(\"rank\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+------+-------+--------------------+-----+-----------+--------------+\n",
      "|_c0| rank|trend|season|episode|                name|start|total_votes|average_rating|\n",
      "+---+-----+-----+------+-------+--------------------+-----+-----------+--------------+\n",
      "|141|82792| null|     1|    142|Ransen Hisshi! We...| 2003|        145|           6.8|\n",
      "|142|71522| null|     1|    143|Soshite Densetsu ...| 2003|        150|           7.5|\n",
      "|195|77786| null|     1|    196|Hijoujitai Hatsur...| 2004|        143|           7.2|\n",
      "|196|75763| null|     1|    197|Ryourinin Sanji! ...| 2004|        141|           7.4|\n",
      "|203|77161| null|     1|    204|Ougon Dakkan Saku...| 2004|        137|           7.4|\n",
      "|206|74726| null|     1|    207|Long Ring Long La...| 2004|        144|           7.4|\n",
      "|207|74069| null|     1|    208|Foxy Kaizoku Dan ...| 2004|        146|           7.4|\n",
      "|208|73811| null|     1|    209|Dai Ikkaisen! Gur...| 2004|        143|           7.5|\n",
      "|209|74363| null|     1|    210|Gingitsune no Fox...| 2004|        149|           7.3|\n",
      "|210|70927| null|     1|    211|Dai 2 Kaisen! Buc...| 2004|        152|           7.5|\n",
      "|211|70764| null|     1|    212|Red Card Renpatsu...| 2004|        145|           7.7|\n",
      "|212|76369| null|     1|    213|Dai Sankaisen! Gu...| 2004|        143|           7.3|\n",
      "|213|73902| null|     1|    214|Hakunetsu Bakusou...| 2004|        139|           7.6|\n",
      "|214|78399| null|     1|    215|Unaru Netsukyuu G...| 2005|        145|           7.1|\n",
      "|215|78146| null|     1|    216|Dangai no Kessen!...| 2005|        146|           7.1|\n",
      "|216|74742| null|     1|    217|Captain Taiketsu!...| 2005|        152|           7.2|\n",
      "|217|74364| null|     1|    218|Zenkai Noro Noro ...| 2005|        149|           7.3|\n",
      "|218|72241| null|     1|    219|Souzetsu Nettou C...| 2005|        144|           7.6|\n",
      "|219|79032| null|     1|    220|Ushinatta? Ubawar...| 2005|        132|           7.4|\n",
      "|220|82396| null|     1|    221|Fue o Idaita Nazo...| 2005|        134|           7.1|\n",
      "+---+-----+-----+------+-------+--------------------+-----+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(F.col(\"trend\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(F.col(\"trend\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.filter(F.col(\"trend\")==\"-\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(F.col(\"total_votes\").isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in trend column as \"-\" have been converted to null, which is okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2940"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.agg(F.min(\"rank\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126450"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.agg(F.max(\"rank\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column _rank_ is not very meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(season=1, count(episode)=958)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output1 - Season-wise\n",
    "df.groupBy(\"season\").agg({\"episode\" : \"count\"}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|season|\n",
      "+------+\n",
      "|     1|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"season\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like _season_ column is not very meaningful either. \n",
    "We cant use it for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------+---------------------+-----------------+\n",
      "|start|no_episodes|total_votes|avg_votes_per_episode|avg_rating       |\n",
      "+-----+-----------+-----------+---------------------+-----------------+\n",
      "|2003 |37         |5784       |156.32432432432432   |7.64054054054054 |\n",
      "|2007 |45         |6707       |149.04444444444445   |7.600000000000002|\n",
      "|2018 |47         |4433       |94.31914893617021    |7.919148936170211|\n",
      "|2015 |48         |6291       |131.0625             |8.202083333333334|\n",
      "|2006 |38         |5666       |149.10526315789474   |7.894736842105263|\n",
      "|2013 |48         |5704       |118.83333333333333   |7.416666666666667|\n",
      "|2014 |50         |6161       |123.22               |7.828000000000003|\n",
      "|2019 |49         |7514       |153.3469387755102    |7.908163265306124|\n",
      "|2004 |39         |5866       |150.4102564102564    |7.753846153846155|\n",
      "|2020 |41         |6483       |158.1219512195122    |7.887804878048779|\n",
      "+-----+-----------+-----------+---------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Yearwise summary\n",
    "df.createOrReplaceTempView(\"df_sql\")\n",
    "query = \"\"\"\n",
    "select \n",
    "    start,\n",
    "    count(episode) as no_episodes,\n",
    "    sum(total_votes) as total_votes,\n",
    "    sum(total_votes) / count(episode) as avg_votes_per_episode,\n",
    "    avg(average_rating) as avg_rating\n",
    "from df_sql\n",
    "group by start\n",
    "\"\"\"\n",
    "spark.sql(query).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "output_file = \"./outputs/year_wise_summary\"\n",
    "spark.sql(query).write.mode(\"overwrite\").parquet(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "raw_partitioned_file = \"./outputs/raw_data\"\n",
    "df.write.partitionBy(\"start\").mode(\"overwrite\").parquet(raw_partitioned_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
